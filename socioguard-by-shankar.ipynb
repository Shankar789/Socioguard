{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project Title : Socioguard\n* **Project Description : GenAI for Social Media  Real-Time Post Moderation**\n* **Developer : Shankar**\n  \n**GenAI Capabilities covered in this projectÂ :**\n1. Structured output/JSON mode/controlled generation\n2. Few-shot prompting\n3. Function Calling\n4. Agents & LangGraph\n5. Embeddings\n6. Retrieval augmented generation (RAG)\n7. Vector search/vector store/vector database","metadata":{}},{"cell_type":"markdown","source":"# Problem & UseÂ Case\nEvery second, social media platforms are flooded with new posts. While most are harmless, some can be toxic, harmful, sensitive, or even life-threatening. Moderating such content manually is slow and expensive, and traditional keyword-based filters often fail to understand the nuance behind a post.\nAs a developer passionate about creating safer digital environments, I built a real-time GenAI-powered text moderation system to classify and take appropriate action on harmful posts & Emergency postsâ€Š-â€Šentirely autonomously, and in seconds.\n\n\nDespite platform efforts, harmful content still makes it throughâ€Š-â€Šfrom bullying and harassment to urgent cries for help.\n  Here's why:\n1. **Manual moderation** can't keep up with the volume of posts being published every second.\n2. **Rule-based systems** are too rigid, missing posts hidden behind slang, emojis, or indirect language.\n3. **Delays in action** can lead to real-world consequencesâ€Š-â€Šfrom mental health impacts to brand damage.\n\nSocial platforms, in particular, require immediate support to detect and automatically respond to:\n* Life-threatening or harmful messages\n* Toxic language and hate speech\n* Sensitive or adult-oriented content\n* Spam, scams, and promotional abuse\n\nThe stakes are highâ€Š-â€Šuser safety, trust, and platform reputation all hang in the balance.\n","metadata":{}},{"cell_type":"markdown","source":"# SolutionÂ \n**How GenAI Solves This in Real Time :**\nTo solve this, I built a fully automated GenAI-based moderation pipeline that understands context, tone, risk, and organizational guidelines before taking action.\n\n**Here's how it works Step-by-Step Breakdown:**\n* User submits a post.\n* The system retrieves relevant organization guidelines using RAG from ChromaDB.\n* The LLM (Gemini Flash) classifies the post into one of ten categoriesÂ : LIFE_EMERGENCY, THREAT, HARASSMENT, OFFENSIVE_LANGUAGE, ADULT, SENSITIVE, PRIORITY_SUPPORT, PROMOTIONAL_CONTENT, SPAM, or NORMAL.\n* The model then analyzesâ€Š-â€ŠPost Tone, Sentiment, Applicable Rule IDÂ ,DescriptionÂ , Hashtags and a Risk Score (0.0â€“1.0)\n* Based on the risk level and category, a moderation agent decides on the appropriate action:\n\n1. Sanitize harmful, sensitive, offensive statements\n2. Normalize or rephrase hateful & threatening language\n\n* Once a post is analyzed and categorized, it swiftly applies the exact action needed based on the severity of the risk, with zero human delay. Here's how the Action Applier takes command using tools available to it:\n\n  ğŸ”´ Critical: Block the user, Remove post & Send the post for urgent review.\n\n  ğŸŸ  High: Block the post, temporarily block user (â‰¤10 mins), queue for review\n\n  ğŸŸ¡ Medium: Temporarily block user (â‰¤10 mins) show cleaned version(Sanitize or normalize the post) with/without any review\n\n  âšª Others: Escalate to emergency, review, or security teams with appropriate priority without any block.\n\n* The system then takes the output from the moderation model and generates a user-friendly explanation, detailing what went wrong and why specific actions were taken.\n\n* The app respects user status such as active, temporarily blocked, or fully blocked, and is designed to operate in real-time (EgÂ : blocked user can not post anything other then Emergency & High priority support). All post data is stored in SQLite tables, and based on specific flags, the app updates the feed accordingly. Each time a user submits a post, the feed refreshes automatically to display all available posts in real-time.","metadata":{}},{"cell_type":"markdown","source":"# ğŸ“¦ Installing required packages\nThese are the core libraries powering our GenAI-based moderation system.\n- google-genai: For Gemini model access and tool-calling\n- chromadb: To store and retrieve moderation guidelines using vector search\n- langgraph: For building multi-step agent workflows\n- tabulate: For neat table outputs","metadata":{}},{"cell_type":"code","source":"# Installing required packages\n!pip install -U -q \"google-genai==1.7.0\"\n!pip install -U -q \"chromadb\"\n!pip install -U -q \"langgraph\"\n!pip install -U -q \"tabulate\"","metadata":{"cellView":"form","id":"WSWhFqmCwIQL","jupyter":{"source_hidden":true},"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:49:10.825364Z","iopub.execute_input":"2025-04-13T15:49:10.825726Z","iopub.status.idle":"2025-04-13T15:49:27.480506Z","shell.execute_reply.started":"2025-04-13T15:49:10.825698Z","shell.execute_reply":"2025-04-13T15:49:27.479254Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# ğŸ“¥ Importing libraries\nThese modules are essential for building and running our moderation system:\n- Google GenAI: For accessing Gemini models and tool-calling\n- ChromaDB: To handle vector embeddings and store moderation guidelines\n- LangGraph: To create structured agent workflows\n- SQLite3 & datetime: For managing post data and timestamps\n- Tabulate & IPython Display: For pretty outputs and inline visuals\n- Typing & Enum: For data structuring and control flow","metadata":{}},{"cell_type":"code","source":"#  Importing libraries\nfrom google.genai import types\nimport typing_extensions as typing\nimport json\nimport enum\nfrom chromadb import Documents, EmbeddingFunction, Embeddings\nfrom google.api_core import retry\nfrom google.genai import types\nimport chromadb\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Optional\nfrom html import unescape\nimport sqlite3\nimport datetime\nfrom google import genai\nfrom IPython.display import HTML, Markdown, display\nfrom google.api_core import retry\nfrom datetime import datetime\nfrom tabulate import tabulate","metadata":{"cellView":"form","id":"cS0DY5MbGh2Z","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:49:27.482203Z","iopub.execute_input":"2025-04-13T15:49:27.482557Z","iopub.status.idle":"2025-04-13T15:49:29.263695Z","shell.execute_reply.started":"2025-04-13T15:49:27.482528Z","shell.execute_reply":"2025-04-13T15:49:29.262764Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# ğŸ” Setting up API Key & Retrying Logic\n - Defines a retry mechanism to handle temporary API errors (e.g., rate limits, service unavailability)\n - Retrieves the Google API key securely using Kaggle secrets\n - Initializes the GenAI client with the authenticated key\n","metadata":{}},{"cell_type":"code","source":"#Setting up API Key & Retrying Logic\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\ngenai.models.Models.generate_content = retry.Retry(\n    predicate=is_retriable)(genai.models.Models.generate_content)\n\nfrom kaggle_secrets import UserSecretsClient\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n# from google.colab import userdata\n# GOOGLE_API_KEY = userdata.get('GEMINI_KEY')\n\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"cellView":"form","id":"5TbVxA_awSEE","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:49:29.264775Z","iopub.execute_input":"2025-04-13T15:49:29.265364Z","iopub.status.idle":"2025-04-13T15:49:29.617194Z","shell.execute_reply.started":"2025-04-13T15:49:29.265333Z","shell.execute_reply":"2025-04-13T15:49:29.616129Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# ğŸ§± Defining Core Types and Enums\n* These TypedDicts and Enums define the structure and allowed values for:\n   \n1. Moderation flow state (`ModerationState`)\n2. Post sentiment (`PostAvailableSentiment`)\n3. Post tone (`PostAvailableTone`)\n4. Category classification (`PostAvilableCategory`)\n5. Risk scoring (`RiskScaleEnum`)\n6. Final moderation decisions and user feedback formats\n\n* These are essential for building a structured and consistent moderation pipeline.","metadata":{}},{"cell_type":"code","source":"#Defining Core Types and Enums\nclass ModerationState(TypedDict):\n    user_post: str\n    validated_post: Optional[str]\n    categorized_post: Optional[str]\n    actioned_post: Optional[str]\n    final_output: Optional[str]\n\nclass PostAvailableSentiment(enum.Enum):\n    POSITIVE = \"POSITIVE\"\n    NEUTRAL = \"NEUTRAL\"\n    NEGATIVE = \"NEGATIVE\"\n    MIXED = \"MIXED\"\n\nclass PostAvailableTone(enum.Enum):\n    EXCITED = \"EXCITED\"\n    HAPPY = \"HAPPY\"\n    CALM = \"CALM\"\n    ANGRY = \"ANGRY\"\n    SAD = \"SAD\"\n    SARCASTIC = \"SARCASTIC\"\n    FEARFUL = \"FEARFUL\"\n    HOPEFUL = \"HOPEFUL\"\n    INFORMATIVE = \"INFORMATIVE\"\n    NEUTRAL = \"NEUTRAL\"\n    ANXIOUS = \"ANXIOUS\"\n    CONFUSED = \"CONFUSED\"\n    FRUSTRATED = \"FRUSTRATED\"\n    AGGRESSIVE = \"AGGRESSIVE\"\n    SUPPORTIVE = \"SUPPORTIVE\"\n    GRATEFUL = \"GRATEFUL\"\n    HUMOROUS = \"HUMOROUS\"\n\nclass PostAvilableCategory(enum.Enum):\n  PRIORITY_SUPPORT = 'PRIORITY_SUPPORT'\n  LIFE_EMERGENCY = 'LIFE_EMERGENCY'\n  SPAM = 'SPAM'\n  HARASSMENT = 'HARASSMENT'\n  ADULT = 'ADULT'\n  SENSITIVE = 'SENSITIVE'\n  THREAT = 'THREAT'\n  NORMAL = 'NORMAL'\n  OFFENSIVE_LANGUAGE = 'OFFENSIVE_LANGUAGE'\n  PROMOTIONAL_CONTENT = 'PROMOTIONAL_CONTENT'\n\nclass RiskScaleEnum(enum.Enum):\n  VERY_LOW = \"0.0\"\n  LOW = \"0.1\"\n  MEDIUM_LOW = \"0.2\"\n  MEDIUM = \"0.3\"\n  MEDIUM_HIGH = \"0.4\"\n  HIGH = \"0.5\"\n  VERY_HIGH = \"0.6\"\n  EXTREMELY_HIGH = \"0.7\"\n  NEAR_PERFECT = \"0.8\"\n  PERFECT = \"0.9\"\n  MAX = \"1.0\"\n\nclass PostCategoryResponse(typing.TypedDict):\n  category: PostAvilableCategory\n  risk_scale: RiskScaleEnum\n  organization_standards_applied: str\n  organization_standards_applied_desc: str\n  post_tone: PostAvailableTone\n  post_sentiment: PostAvailableSentiment\n  auto_hash_tag: str\n\nclass NormalizedTextResponse(typing.TypedDict):\n  normalized_post: str\n\nclass SanitizedTextResponse(typing.TypedDict):\n  sanitized_post: str\n\nclass ReviewActionsAppliedAndNotifyUserResponse(typing.TypedDict):\n  final_response_to_user_on_post_submission: str\n  any_error_during_actions: bool\n\nclass DataBaseName:\n  DATABASE_NAME = \"post_management.db\"\n  @staticmethod\n  def get_database_name():\n    return DataBaseName.DATABASE_NAME\n","metadata":{"cellView":"form","id":"PZoZ5NygGxer","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:49:29.619221Z","iopub.execute_input":"2025-04-13T15:49:29.620029Z","iopub.status.idle":"2025-04-13T15:49:29.631846Z","shell.execute_reply.started":"2025-04-13T15:49:29.619992Z","shell.execute_reply":"2025-04-13T15:49:29.630605Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# âš™ï¸ Model & Configuration Setup\n\n* Define the Gemini model version used (`GOOGLE_FLASH_V2`)\n* Provide reusable configuration presets for different moderation tasks:\n\n1. Categorization, normalization, sanitization\n2. Action triggering with tool calling\n3. RAG-based decision making with guideline retrieval\n4. User feedback after moderation\n\n* Helps maintain consistency and modularity across all model calls.","metadata":{}},{"cell_type":"code","source":"#Model & Configuration Setup\nclass Models:\n  GOOGLE_FLASH_V2 = \"gemini-2.0-flash\"\n  @staticmethod\n  def get_model():\n    return Models.GOOGLE_FLASH_V2\n\n\nclass ModelConfig:\n  @staticmethod\n  def get_category_model_config():\n    return types.GenerateContentConfig(max_output_tokens=150,\n                                           temperature=0.1,\n                                           top_p=0.95,\n                                           response_mime_type=\"application/json\",\n                                           response_schema=PostCategoryResponse)\n  @staticmethod\n  def get_normalize_model_config():\n    return types.GenerateContentConfig(temperature=0.1,\n                                           top_p=0.95,\n                                           response_mime_type=\"application/json\",\n                                           response_schema=NormalizedTextResponse)\n  @staticmethod\n  def get_sanitize_model_config():\n    return types.GenerateContentConfig(temperature=0.1,\n                                           top_p=0.95,\n                                           response_mime_type=\"application/json\",\n                                           response_schema=SanitizedTextResponse)\n  @staticmethod\n  def get_action_model_config(add_review_entry,block_user,insert_post_visibility_status):\n    return types.GenerateContentConfig(temperature=0.1,\n                                      top_p=0.95,\n                                      tools=[add_review_entry, block_user,insert_post_visibility_status])\n  @staticmethod\n  def get_rag_based_action_model_config(add_review_entry,block_user,insert_post_visibility_status,retrive_guidelines_for_actions):\n    return types.GenerateContentConfig(temperature=0.1,\n                                      top_p=0.95,\n                                      tools=[add_review_entry, block_user,insert_post_visibility_status, retrive_guidelines_for_actions])\n  @staticmethod\n  def get_review_action_applied_and_notify_user_model_config():\n    return types.GenerateContentConfig(temperature=0.1,\n                                           top_p=0.95,\n                                           response_mime_type=\"application/json\",\n                                           response_schema=ReviewActionsAppliedAndNotifyUserResponse)\n","metadata":{"cellView":"form","id":"lXfEcbRoG_Lb","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:49:29.632827Z","iopub.execute_input":"2025-04-13T15:49:29.633161Z","iopub.status.idle":"2025-04-13T15:49:29.653835Z","shell.execute_reply.started":"2025-04-13T15:49:29.633129Z","shell.execute_reply":"2025-04-13T15:49:29.652938Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"\n# ğŸ“„ Prompt Templates for All Moderation Tasks\n \n This class contains all the prompt templates used for different stages of moderation:\n \n 1. ğŸš¨ `CATEGORY_ZERO_SHOT_PROMPT`: Classifies user input into predefined moderation categories and generates structured metadata (tone, sentiment, risk, etc.)\n 2. ğŸ§¹ `NORMALIZE_FEW_SHOT_PROMPT`: Rewrites harsh/offensive user input into a softer, respectful tone using few-shot examples.\n 3. ğŸ”’ `SANITIZE_FEW_SHOT_PROMPT`: Masks sensitive or harmful content (personal details, medical info, profanity, etc.) using few-shot examples.\n 4. ğŸ›  `APPLY_ACTIONS_NON_RAG_ZERO_SHOT_PROMPT`: Directly triggers moderation actions based on structured input and rules.\n 5. ğŸ“š `ACTIONS_AVILABLE_ZERO_PROMPT`: Defines the complete set of moderation tools available and the conditions under which each tool should be used.\n 6. ğŸ§  `APPLY_ACTIONS_RAG_ZERO_SHOT_PROMPT`: Uses RAG to retrieve action guidelines and applies actions strictly as per organizational policies.\n 7. âœ… `REVIEW_ACTIONS_APPLIED_ZERO_SHOT_PROMPT`: Evaluates moderation actions taken and generates a human-like response back to the user including status, reasoning, and escalation (if any).\n\n âœ… All prompts follow strict output formatting and domain-specific constraints to maintain consistency and reliability during moderation.\n","metadata":{}},{"cell_type":"code","source":"# Prompt Templates for All Moderation Tasks\nclass Prompts:\n  CATEGORY_ZERO_SHOT_PROMPT = \"\"\"\n  You are an AI moderation assistant designed to classify user-generated text into one of the following predefined categories:\n  1. LIFE_EMERGENCY\n  2. THREAT\n  3. HARASSMENT\n  4. OFFENSIVE_LANGUAGE\n  5. PRIORITY_SUPPORT\n  6. SENSITIVE\n  7. ADULT\n  8. PROMOTIONAL_CONTENT\n  9. SPAM\n  10. NORMAL\n\n  Your task is to analyze the user input and return a structured response based on both the input and the provided Organizational Standards (retrieved via RAG).\n\n  ### Your Output Must Include:\n    1. **Category** : Choose the most appropriate category from the list above, based on the following priority order: LIFE_THREATENING_EMERGENCY > THREAT > HARASSMENT > OFFENSIVE_LANGUAGE > PRIORITY_SUPPORT > SENSITIVE > ADULT > PROMOTIONAL_CONTENT > SPAM > NORMAL\n    2. **Risk Scale** : A numeric score between 0.0 (low risk) and 1.0 (high risk), representing the urgency or danger level of the post.\n    3. **Organization Standards Applied for Category** : Provide the name of the rule or standard used to classify the category. If no matching rule is found, return 'Others'.\n    4. **Organization Standards Applied for Category Description** : A short explanation (max 20 words) describing why this category was selected based on the standard.\n    5. **Post Tone** : Describe the emotional tone of the post (e.g., angry, calm, frustrated, urgent, sarcastic).\n    6. **Post Sentiment** : Determine the sentiment expressed (e.g., positive, negative, neutral, mixed).\n    7. **Auto Hash Tag** : Generate up to 3 relevant more professional language hashtags for the post (comma-separated, no # symbol, lowercase).\n\n  ### Instructions and Guidelines:\n  - Always classify the input by following this strict priority order, as users may try to mask high-priority issues within low-priority content.Use the following descending order of importance when assigning categories: LIFE_THREATENING_EMERGENCY > THREAT > HARASSMENT > OFFENSIVE_LANGUAGE > PRIORITY_SUPPORT > SENSITIVE > ADULT > PROMOTIONAL_CONTENT > SPAM > NORMAL\n  - Prioritize using the Organizational Standards. If unclear, rely on logical reasoning.\n  - Make sure to classify the text accurately without raising false alerts or false positives. If the message is neutral, it should be categorized as \"NORMAL.\"\n  - Do not include any special characters in the output except single quotes (').\n  - The Category must always match the Organizational Standard applied.\n  - User inputs may include emojis, grammar mistakes, or informal/slang language. Always judge based on intent and context.\n\n  ### Organizational Standards:\n    {}\n\n  ### User Input:\n    {}\n  \"\"\"\n  NORMALIZE_FEW_SHOT_PROMOT =\"\"\"\n  You are a respectful communication assistant. Rephrase the given message to remove any harassing, aggressive, or offensive language.Make softer tone to give same meaning\n\n  Example 1:\n  Original: \"You're such a loser, nobody wants you here.\"\n  Rephrased: \"Iâ€™m having a hard time connecting with you on this topic.\"\n\n  Example 2:\n  Original: \"Why don't you just shut up already?\"\n  Rephrased: \"Iâ€™d prefer if we paused this conversation for now.\"\n\n  Example 3:\n  Original: \"People like you are the worst!\"\n  Rephrased: \"I strongly disagree with your opinion.\"\n\n  Now rephrase the following message:\n  \"{}\"\n  \"\"\"\n  SANITIZE_FEW_SHOT_PROMPT = \"\"\"\n  You are a sensitive & ofensive words content masking assistant. Your task is to identify and mask any sensitive or offensive language information such as personal details, medical advice, medications,bad words or anything that should not be blindly followed. Replace only the sensitive parts with [MASKED] and keep the rest of the sentence unchanged.\n\n  Examples:\n\n  Example 1:\n  Original: \"My phone number is 9876543210\"\n  Masked: \"My phone number is [MASKED]\"\n\n  Example 2:\n  Original: \"I take Xanax 3 times a day\"\n  Masked: \"I take [MASKED] 3 times a day\"\n\n  Example 3:\n  Original: \"Send your documents to john.doe@example.com\"\n  Masked: \"Send your documents to [MASKED]\"\n\n  Example 4:\n  Original: \"Just drink apple cider vinegar every morning to cure your diabetes\"\n  Masked: \"Just drink [MASKED] every morning to cure your diabetes\"\n\n  Example 5:\n  Original: \"I live at 44-A, Galaxy Apartments, NY\"\n  Masked: \"I live at [MASKED]\"\n\n  Example 5:\n  Original: \"oh fuck, i am screwd\"\n  Masked: \"oh [MASKED], i am screwd\"\n\n  Now, mask the sensitive information in this sentence:\n  \"{}\"\n  \"\"\"\n  APPLY_ACTIONS_NON_RAG_ZERO_SHOT_PROMPT = \"\"\"\n  You are a social media moderator responsible for analyzing user posts and taking appropriate moderation actions.\n  ***Note -\n    1. If you experience any issues or failures while using the tools, please report them in your response.\n    2. DO NOT use any Escape sequences & special charactors in your response text except single quotes (')\n  input:\n  {}\n\n  apply actions:\n  {}\n  \"\"\"\n\n  ACTIONS_AVILABLE_ZERO_PROMPT = \"\"\"\n  You have access to perform only the following actions:\n    1.ACTION NAME: Critical / Block for Review\n    DESC : Block the user until the review team clears them. Set is_blocked_for_review = true,is_post_blocked = true is and escalate to the review team with High priority to prevent the user from posting further until clearance is granted. This action is intended for high-confidence cases where the post clearly violates community guidelines but requires review before further decisions.\n    Conditions: Apply this action only if the risk_scale is 0.9 or higher, except for posts categorized as LIFE_EMERGENCY. ***DO NOT BLOCK USER/POST FOR LIFE_EMERGENCY or PRIORITY_SUPPORT category,\n\n     is_post_waiting_for_clearance |   is_post_visible\n\n    2.ACTION NAME: Temporary Block with Review\n    DESC : Temporarily block the user and involve the review team. The temporary block must not exceed 10 minutes. Set is_blocked = true,is_post_waiting_for_clearance = true,is_post_blocked=false,is_post_visible=false and is_blocked_for_review = false with the appropriate duration and timestamp. Escalate the issue to the review team with Medium priority for further analysis.\n    Conditions: Use this action when risk_scale is 0.8 and the case requires additional review.***DO NOT BLOCK USER/POST FOR LIFE_EMERGENCY or PRIORITY_SUPPORT category\n\n    3.ACTION NAME: Temporary Block without Review\n    DESC : Temporarily block the user for up to 10 minutes without involving the review team. Set is_blocked = true,is_post_visible=true,is_post_blocked=false & is_post_waiting_for_clearance=false with the appropriate duration and timestamp.\n    Conditions: Use this action when risk_scale is 0.7. This block is considered lower risk but might still require escalation later.***DO NOT BLOCK USER FOR LIFE_EMERGENCY or PRIORITY_SUPPORT or PROMOTIONAL_CONTENT categories\n\n    4.ACTION NAME: Non-Blocking Escalation Only\n    DESC :\n       - When user blocking is not necessary, escalate the issue directly to the review, emergency, or security team based on context. Set the correct priority flag (HIGH, MEDIUM, or LOW) depending on the situation. Use the category, risk_scale, post_tone, and post_sentiment values from the input to determine the priority.\n       - This action should be used when the issue needs attention but blocking is not appropriate.\n      Example: In emergency cases where help is already on the way or someone is actively handling the situation, reduce the priority to LOW with is_post_visible=true.\n    Conditions: Apply this action only when escalation is sufficient, and make sure to select the most relevant internal team with right priority undesrtanding the situvation.\n      \"\"\"\n\n  APPLY_ACTIONS_RAG_ZERO_SHOT_PROMPT =\"\"\"\n  Social Media Moderation: Guidelines, Tools, and Decision-Making Process:\n  1.As a social media moderator, your responsibility is to review user posts and decide on the appropriate moderation actions.\n  2.You have access to multiple tools, and you must always use the retrieval tool to access the organization's action guidelines through RAG. Ensure your search string includes risk_scale, category, and any other relevant details that help justify your decision.\n  3.You are not allowed to take any decisions other then ORGANISATION GUIDELINES.\n\n  Input:\n  {}\n\n  Output : what actions taken and any failures while applying actions\n  \"\"\"\n  REVIEW_ACTIONS_APPLIED_ZERO_SHOT_PROMPT = \"\"\"\n  You are an interactive social media moderator. Your task is to review the output of system that applied actions for a post by user and provide feedback on the actions applied by the system. Your response should include details on what went well and what actions were taken\n  Note :\n    1.Except life-threatening emergencies or high-priority support cases , Feel free to add a touch of professional fun. You can tease professionally the user when it comes use of harrasment & Offensive languages\n    2.***DO NOT use any special charactor except single quotes (')\n    3.sometimes, you recive input like talking or questioning statment, Do not get confused with that as your input is from other model parse it properly\n  Your output should include:\n  1. final_response_to_user_on_post_submission: str ->\n      Using the input, generate a precise, concise message that includes only:\n        - refer \"actions_taken\" to find type of block (if any) & is it existing one or new one\n        - refer \"actions_taken\" to find duration of block\n        - refer \"actions_taken\" to find post status any one of this visible,blocked,waiting for clearance\n        - refer \"organization_standards_applied\" or \"organization_standards_applied_desc\" to find reason for actions.\n        - refer \"actions_taken\" escalations if any, if so which team\n  2. any_error_during_actions: bool -> Set this to true if the input text suggests that the system attempted an action but encountered a problem. Otherwise, set it to false..\n\n  Input:\n  {}\n  \"\"\"\n","metadata":{"id":"ouTcg6PZHGWq","cellView":"form","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:49:29.654834Z","iopub.execute_input":"2025-04-13T15:49:29.655112Z","iopub.status.idle":"2025-04-13T15:49:29.678834Z","shell.execute_reply.started":"2025-04-13T15:49:29.655087Z","shell.execute_reply":"2025-04-13T15:49:29.677807Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# ğŸ“„ Organisation Guidelines for Risk Scoring\n\nTo ensure that our content moderation system makes responsible and accurate decisions, we define a structured set of organizational rules. Each rule includes:\n\n- A **Title** describing the context of the guideline.\n- Clear **examples** of text patterns or phrases that match the rule.\n- A breakdown of **Risk Boost levels** (Very Low to Very High) based on severity, urgency, or potential harm.\n\nThese rules act as domain knowledge input to our AI system, helping us compute a **risk score** for each post. This score is later used to determine moderation actions like escalation, blocking, or further review.\n\nBelow is the `OrganisationGuidelinesDocument` class that holds all these rules as raw text for downstream use in the risk computation engine. These can be queried or embedded for use in LLM prompting or similarity search.\n","metadata":{}},{"cell_type":"code","source":"#Organisation Guidelines for Risk Scoring\nclass OrganisationGuidelinesDocument:\n  ORG_GUIDELINES_FOR_RISK =[\n  \"\"\"\n  Rule Title : Urgent Requests or Situations\n  Eaxmple: Text that includes urgent phrases like â€œemergency,â€ â€œurgent,â€ â€œhelp me,â€ â€œI need assistance immediately,â€ â€œsomething happened,â€ â€œaccident,â€ â€œcrisis,â€ â€œcall 911,â€ etc.\n  Risk Boost:\n    1.High Risk: If critical emergency terms are found (e.g., â€œemergency,â€ â€œurgent,â€ â€œaccidentâ€), and the person is unable to receive assistance or is in immediate danger, set the Risk between 0.8 and 1.0.\n    2.Moderate Risk: If the person is in an urgent situation but help is available or manageable (e.g., â€œcall 911â€ or â€œhelp is on the wayâ€), set the Risk between 0.5 and 0.7, adjusting based on the level of urgency and available support.\n    3.Low Risk: If the situation is urgent but the individual is either able to manage it or if the urgency is somewhat less critical (e.g., general urgent inquiries), adjust the Risk score between 0.1 and 0.4.\n  \"\"\",\n  \"\"\"\n  Rule Title : Urgent Life-Threatening Requests\n  Example: Text that includes terms like â€œemergency,â€ â€œhelp me,â€ â€œimmediate danger,â€ â€œaccident,â€ â€œcritical,â€ â€œinjured,â€ etc.\n  Risk Boost:\n    1.Very High Risk (0.9 - 1.0): The text indicates imminent danger or severe distress, requiring urgent intervention (e.g., â€œMy friend is injured, I need help right now, theyâ€™re in critical condition!â€).\n    2.High Risk (0.7 - 0.8): The situation is urgent, and immediate action is needed, but some support is on the way or can be arranged (e.g., â€œMy friendâ€™s in an accident, Iâ€™ve called for help, but they need medical attention urgently!â€).\n    3.Moderate Risk (0.5 - 0.6): The urgency is clear, but the situation can likely be managed by the individual or by receiving help soon (e.g., â€œMy friendâ€™s in danger, we need to act fast to help them!â€).\n    4.Low Risk (0.3 - 0.4): The situation is time-sensitive, but not life-threatening or critical (e.g., â€œMy friend needs help immediately, but the situation isnâ€™t too dangerous, just urgentâ€).\n    5.Very Low Risk (0.1 - 0.2): The urgency is mild, and while help is requested, the situation is not critical or dangerous (e.g., â€œHey, my friendâ€™s in a tough spot, can someone help ASAP?â€).\n  \"\"\",\n  \"\"\"\n  Rule Title : Non-Life-threatening Requests\n  Example: Text that includes terms like â€œurgent,â€ â€œblocked,â€ â€œimmediate help,â€ â€œaccount,â€ â€œaccess issue,â€,\"Hacked\" etc., where the urgency indicates a non-critical situation and no offensive or inappropriate language is used.\n  Risk Boost:\n\t  1.Very High Risk (0.9 - 1.0): Text indicates a high level of urgency and distress over a non-life-threatening issue (e.g., â€œMy friendâ€™s account has been blocked, and they need it unlocked immediately because itâ€™s preventing them from working!â€).\n\t  2.High Risk (0.7 - 0.8): The request expresses urgency about a non-life-threatening issue, but the situation can be resolved soon with help (e.g., â€œMy friendâ€™s account is blocked, can someone help fix this ASAP?â€).\n\t  3.Moderate Risk (0.5 - 0.6): The urgency is implied but not critical, and the person can likely resolve it soon (e.g., â€œMy friendâ€™s account has been blocked, but I think itâ€™s not a big deal, just need some helpâ€).\n\t  4.Low Risk (0.3 - 0.4): The urgency is mild or non-urgent (e.g., â€œMy friendâ€™s account is blocked, but Iâ€™m sure it will be sorted soonâ€).\n\t  5.Very Low Risk (0.1 - 0.2): The request is an inquiry or casual mention with no significant urgency (e.g., â€œHey, my friendâ€™s account was blocked. Any idea how to fix it?â€).\n  \"\"\",\n  \"\"\"\n  Rule Title : Promotional Content\n  Eaxmple: Text containing terms like â€œlimited time offer,â€ â€œfree,â€ â€œbuy now,â€ â€œsave big,â€ â€œdiscount,â€ â€œclick hereâ€.\n  Risk Boost:\n    1.High Risk: If the promotional content appears potentially misleading, manipulative, or suspicious (e.g., aggressive sales tactics or offers that promise unrealistic results), set the Risk between 0.7 and 1.0.\n    2.Moderate Risk: If the promotional content seems suspicious, such as offers that feel a bit too good to be true but are not overtly misleading (e.g., some ambiguous or vague claims like â€œlimited time offerâ€ with no clear details), set the confidence between 0.4 and 0.6.\n    3.Low Risk: If the promotional content appears legitimate, such as clear and honest offers (e.g., genuine discounts or sales), set the Risk between 0.1 and 0.3.\n  \"\"\",\n  \"\"\"\n  Rule Title : Repetitive Phrases\n  Eaxmple: If the text contains repetitive phrases or call-to-action commands such as â€œhurry up,â€ â€œdonâ€™t miss out,â€ â€œact nowâ€, â€œorder todayâ€.\n  Risk Boost:\n    1.High Risk: If the repetitive phrases are identified as manipulative, spammy, or overly aggressive (e.g., excessive use of phrases like â€œhurry upâ€ or â€œact nowâ€ with the intent to pressure or manipulate), set the Risk between 0.7 and 1.0.\n    2.Moderate Risk: If the repetitive phrases seem mildly persuasive or slightly manipulative but not overly aggressive (e.g., moderate use of phrases like â€œdonâ€™t miss outâ€ or â€œorder todayâ€ in promotional content), set the Risk between 0.4 and 0.6.\n    3.Low Risk: If the repetitive phrases are used in a non-intrusive or legitimate way (e.g., occasional reminders or general calls to action), set the Risk between 0.1 and 0.3.\n  \"\"\",\n  \"\"\"\n  Rule Title : Links and Advertising\n  Eaxmple: Text with links, advertisements, or unsolicited promotional content.\n  Risk Boost:\n    1.High Risk: If the text contains unsolicited, aggressive, or suspicious promotional content or external links (e.g., spam, excessive advertising, or links leading to questionable sites), set the Risk between 0.7 and 1.0.\n    2.Moderate Risk: If the links or ads are somewhat promotional or unsolicited but not overly aggressive (e.g., links to promotional content from a trusted company), set the Risk between 0.4 and 0.6.\n    3.Low Risk: If the text contains legitimate, non-intrusive, and relevant links or advertisements (e.g., trusted sources, non-spammy offers), set the Risk between 0.1 and 0.3.\n  \"\"\",\n  \"\"\"\n  Rule Title: Harassment Through Insults\n  Example: Text that contains repeated or targeted insults, such as â€œstupid,â€ â€œidiot,â€ â€œuseless,â€ â€œpathetic,â€ or â€œawful person,â€ especially when aimed at demeaning or belittling an individual over time.\n  Risk Boost:\n    1.High Risk: If the text contains persistent or aggressive insults, or a combination of insults that seem aimed at harassing or degrading the individual, set the Risk between 0.7 and 1.0.\n    2.Moderate Risk: If the text contains moderate insults that could be perceived as part of a pattern of harassment but are not excessively harsh, set the Risk between 0.4 and 0.6.\n    3.Low Risk: If the text contains isolated or subtle insults that do not seem to indicate a sustained attack or repeated behavior, set the Risk between 0.1 and 0.3.\n  \"\"\",\n   \"\"\"\n    Rule Title : Insults Directed at an Individual\n    Example: Text that contains repeated or targeted insults, such as â€œstupid,â€ â€œidiot,â€ â€œuseless,â€ â€œpathetic,â€ or â€œawful person,â€ especially when aimed at demeaning or belittling an individual over time.\n    Risk Boost:\n      1. Very High Risk (0.9 - 1.0): Continuous and aggressive insults or bullying directed at an individual, creating a harmful or toxic environment.Eg: \"You should just leave. No one wants you here, ever.\"\n      2. High Risk (0.7 - 0.8): Frequent or harsh insults aimed at an individual, though not overtly malicious or bullying in nature.Eg: \"Youâ€™re such a loser, no one cares about your opinion.\"\n      3. Moderate Risk (0.5 - 0.6): Insults are present but not severe; they may suggest tension or discomfort but are not constant or overly harsh.Eg: \"I canâ€™t believe how dumb you are. Youâ€™ll never get it.\"\n      4. Low Risk (0.3 - 0.4): Minor insults, typically isolated or not directed at an individual in an aggressive way.Eg: \"You should really start listening to people for once.\"\n      5. Very Low Risk (0.1 - 0.2): Insults or negative comments used casually, without targeting or harming anyone intentionally.Eg: \"You're always so annoying, but Iâ€™ll still help you out.\"\n    \"\"\",\n  \"\"\"\n  Rule Title : Aggressive Tone\n  Eaxmple: If the text includes aggressive phrases like â€œshut up,â€ â€œleave me alone,â€ â€œshut your mouth,â€ â€œI canâ€™t stand youâ€.\n  Risk Boost:\n    1.High Risk: If the text contains highly aggressive or confrontational language (e.g., harsh or multiple aggressive phrases), set the Risk between 0.8 and 1.0.\n    2.Moderate Risk: If the text has moderate aggression (e.g., multiple phrases or slightly stronger language), set the Risk between 0.5 and 0.7.\n    3.Low Risk: If the text includes subtle or mild aggressive phrases (e.g., a single phrase with less intensity), set the Risk between 0.1 and 0.4.\n  \"\"\",\n  \"\"\"\n  Rule Title : Threatening Words\n  Eaxmple: Text that includes threats but in a more subtle form like â€œyouâ€™ll regret this,â€ â€œyouâ€™ll pay for this,â€ â€œyouâ€™ll wish you hadnâ€™t done that.â€\n  Risk Boost:\n    1.High Risk: If the text strongly implies or suggests harassment or a serious threat (but still not explicit), set the Risk to 0.9.\n    2.Moderate Risk: If the threats are more direct or suggest mild harassment without being overtly harmful, set the Risk between 0.5 and 0.7.\n    3.Low Risk: If the threatening words are mild or imply harm in a non-confrontational way (e.g., subtle hints with no direct threat), set the Risk between 0.1 and 0.4.\n  \"\"\",\n  \"\"\"\n  Rule Title : Explicit Sexual Content\n  Eaxmple: Text that includes sexually explicit content or adult material such as terms like â€œsex,â€ â€œsexual act,â€ â€œporn,â€ â€œadult film,â€ â€œadult content,â€ â€œeroticâ€.\n  Risk Boost:\n    1.High Risk: If the content contains clear, explicit sexual terms or graphic adult material, set the Risk to 1.0.\n    2.Moderate Risk: If the content includes somewhat explicit references or alludes to adult material in a non-graphic manner (e.g., vague or indirect references), set the Risk between 0.5 and 0.7.\n    3.Low Risk: If the content contains mild references or euphemisms related to sexual themes without explicit details (e.g., casual mentions), set the Risk between 0.1 and 0.4.\n  \"\"\",\n  \"\"\"\n  Rule Title : Sexual Implications or Innuendo\n  Eaxmple: Text that includes sexual innuendo or implied sexual references like â€œhookup,â€ â€œmake love,â€ â€œget intimate,â€ â€œone-night stand,â€ â€œflirtâ€ and indirect references to sex organ are considered Adult catogery.\n  Risk Boost:\n    1.High Risk: If the content explicitly refers to sexual activity or intimate situations, or clearly represents a sexual act, the Risk should be set between 0.8 and 1.0.\n    2.Moderate Risk: If the content includes implied or indirect sexual activity or intimacy, or indirectly suggests sexual acts or encounters, set the Risk between 0.5 and 0.7.\n    3.Low Risk: If the content contains casual or light sexual implications, flirtatious comments, or subtle references that could be interpreted as suggestive but are not directly explicit (such as mild flirting or indirect allusions to sexual organs), set the Risk between 0.1 and 0.4.\n  \"\"\",\n  \"\"\"\n  Rule Title : Adult Conversations or Topics\n  Eaxmple: Text discussing adult relationships, marital issues, or extramarital affairs in a sexual context, such as â€œaffair,â€ â€œcheating,â€ â€œunfaithful,â€ â€œlove affairâ€.\n  Risk Boost:\n    1.High Risk: If the content involves explicit discussions of infidelity, sexual relationships, or sexual undertones (e.g., explicit mention of extramarital affairs with sexual connotations), set the Risk to 0.9.\n    2.Moderate Risk: If the text discusses adult relationships with mild or implied sexual references (e.g., general discussion of marital issues or infidelity without overt sexual content), set the Risk between 0.5 and 0.7.\n    3.Low Risk: If the text discusses adult topics or relationships in a neutral, non-sexual context (e.g., casual mention of relationships or marriage without explicit or sexual undertones), set the Risk between 0.1 and 0.4.\n  \"\"\",\n  \"\"\"\n  Rule Title : PII (Personally Identifiable Information)\n  Eaxmple: Text containing keywords like â€œsocial security number,â€ â€œbank account,â€ â€œaddress,â€ â€œphone number,â€ â€œemail address,â€ â€œdriver's licenseâ€.\n  Risk Boost:\n    1.High Risk: If explicit, clear references to sensitive PII (e.g., full social security numbers, bank account details, unmasked addresses, etc.) are found, set the Risk to 1.0.\n    2.Moderate Risk: If the text includes partial or obscured references to PII, such as a phone number with dashes or a partially redacted address (e.g., â€œ1234**** streetâ€), set the Risk between 0.5 and 0.7.\n    3.Low Risk: If the text contains any indirect references to PII, such as vague or non-specific terms related to personal information (e.g., â€œcontact info,â€ â€œpersonal detailsâ€), set the Risk between 0.1 and 0.4.\n  \"\"\",\n  \"\"\"\n  Rule Title : Financial or Legal Information\n  Eaxmple: Text that includes â€œaccount number,â€ â€œbalance,â€ â€œtransaction,â€ â€œpayment,â€ â€œlegal caseâ€, or anything involving money or legal proceedings.\n  Risk Boost:\n    1.High Risk: If the text includes explicit financial or legal information that could be harmful, such as full account numbers, legal proceedings details, or confidential financial data (e.g., â€œmy account number is 123-456-789â€), set the Risk to 0.9 or higher.\n    2.Moderate Risk: If the text includes references to financial or legal topics that appear more specific or potentially sensitive but not clearly malicious (e.g., â€œmy bank account is being reviewedâ€ or â€œa legal case is pendingâ€), set the Risk between 0.5 and 0.7.\n    3.Low Risk: If the text mentions financial or legal terms in a casual or non-sensitive context, such as a discussion about a transaction or payment in a general context (e.g., â€œI made a payment todayâ€), set the Risk between 0.1 and 0.4.\n  \"\"\",\n  \"\"\"\n  Rule Title : Medical Information\n  Eaxmple: Text containing terms like â€œdiagnosis,â€ â€œmedication,â€ â€œtreatment,â€ â€œsurgeryâ€ that could be considered sensitive, especially when providing advice or instructions that should not be blindly followed.\n  Risk Boost:\n    1.High Risk: If the text involves sensitive or explicit medical information, such as diagnoses, instructions, or advice that could be harmful if blindly followed (e.g., â€œTake this medication without consulting a doctorâ€ or â€œHereâ€™s a treatment plan for your conditionâ€), set the confidence to 0.9 or higher. Ensure that such content is flagged for caution.\n    2.Moderate Risk: If the text contains more specific details related to medical conditions or treatments, but not with explicit instructions or harmful advice (e.g., â€œIâ€™m on medication for my conditionâ€), set the Risk between 0.5 and 0.7.\n    3.Low Risk: If the medical information is shared in a general or non-specific context, with no explicit guidance or advice (e.g., â€œI went to the doctorâ€ or â€œIâ€™m considering a treatmentâ€), set the Risk score between 0.1 and 0.4.\n  \"\"\",\n  \"\"\"\n  Rule Title : Explicit Threats\n  Eaxmple: Text containing explicit threats such as â€œI will kill you,â€ â€œI will harm you,â€ â€œIâ€™m going to hurt youâ€,\"i have planted a bomb in stadium\".\"conducting/supporting terror attacks\",\"Bomb will blast in 2 min\"\n  Risk Boost:\n    1.High Risk: If the text contains clear, direct, and explicit threats of harm, such as â€œI will kill youâ€ or â€œIâ€™m going to hurt you,â€ set the Risk to 1.0. This should be flagged as a severe threat that requires immediate attention.\n    2.Moderate Risk: If the threat is less direct but still implies harm (e.g., â€œYouâ€™re going to regret thisâ€ or â€œIâ€™ll make you pay for thisâ€), set the Risk between 0.7 and 0.9 based on the context and level of implied danger.\n    3.Low Risk: If the text contains ambiguous or less clear threats that donâ€™t convey immediate danger (e.g., â€œIâ€™ll deal with you laterâ€ or â€œYouâ€™ll regret thisâ€), set the Risk between 0.3 and 0.6 depending on the language and context.\n  \"\"\",\n  \"\"\"\n  Rule Title : Implicit Threats\n  Eaxmple: Text that subtly threatens harm, like â€œYouâ€™ll regret this,â€ â€œYouâ€™ll pay for this,â€ â€œYou donâ€™t know who youâ€™re dealing with.â€\n  Risk Boost:\n    1.High Risk: If the text subtly implies harm with strong language or context (e.g., â€œYouâ€™ll pay for thisâ€ or â€œYou donâ€™t know who youâ€™re dealing withâ€), set the Risk to 0.7 to 0.8. This should be flagged as a serious potential threat.\n    2.Moderate Risk: If the text contains less direct but still concerning phrases (e.g., â€œYouâ€™ll regret thisâ€), set the Risk to 0.4 to 0.6 based on the severity of the language used and the potential for harm.\n    3.Low Risk: If the text contains mild or unclear hints of a threat (e.g., â€œYouâ€™ll see what happensâ€ or â€œThis wonâ€™t be forgottenâ€), set the Risk to 0.1 to 0.3. These should be considered for review but may not indicate a high level of threat.\n  \"\"\",\n  \"\"\"\n  Rule Title : Neutral and Non-sensitive Content\n  Eaxmple: Text without any emotionally charged language or sensitive information. This includes casual conversations, neutral statements, and general inquiries (e.g., â€œHow are you?â€, â€œWhat time is the meeting?â€, â€œLetâ€™s have lunch tomorrowâ€).\n  Risk Boost:\n    1.High Risk: For entirely neutral, non-sensitive content with no emotionally charged or harmful language, set the Risk to 1.0. This content can be classified as completely safe and non-urgent.\n    2.Moderate Risk: In rare cases where the tone or language could be perceived as slightly emotional but still neutral (e.g., light humor or excitement), set the Risk to 0.7 to 0.9. This is still considered low-risk, but it may require closer attention.\n    3.Low Risk: If the content is borderline neutral and contains minor emotional cues that could be misinterpreted, set the Risk to 0.4 to 0.6 for further review, but it should still be flagged as safe unless stronger indicators arise.\n  \"\"\",\n  \"\"\"\n  Rule Title: Offensive Language\n  Example: Text containing profanity or bad words in a casual or non-threatening context, such as â€œWhat the hell,â€ â€œdamn it,â€ or â€œscrew this.â€,\"fuck it, what can i do now\",\"oh shit, how can i do this\"\n  Risk Boost:\n    1.High Risk: If the bad words used in statment then mark it to 0.8 , language used in a highly inappropriate or aggressive way, but still doesnâ€™t escalate to harassment or abuse, set the Risk to 0.8 to 1.0.\n    2.Moderate Risk: If bad words are used in a frustrated but non-aggressive context (e.g., slight irritation), set the Risk between 0.5 and 0.7.\n    3.Low Risk: If the language is used mildly and does not escalate to any form of insult or aggression, set the Risk between 0.1 and 0.4.\n  \"\"\",\n  \"\"\"\n  Rule Title: Others\n  Examples & Actions:\n    i. If Category is in [EMERGENCY, THREAT] â†’ Use your best judgment to make decisions freely based on your understanding.\n    ii. If Category is in [SPAM, HARASSMENT, ADULTERY, SENSITIVE, NORMAL] â†’ Be cautious; avoid false alerts and unnecessary flags.\n  Risk Boost: Adjust the Risk score dynamically between 0.1 to 1.0 based on your level of certainty and contextual understanding.\n  \"\"\"\n  ]\n  ORG_GUIDELINES_FOR_ACTIONS =[\n      \"\"\"\n      ACTION NAME : Critical / Block for Review\n      - Block the user pending review: Block the user until the review team clears them. Set `is_blocked_for_review = true` and escalate to the review team to prevent the user from posting further until clearance is given. This action is for high-confidence cases, typically when the post violates community guidelines but requires review before deciding on further action.\n        - ***Conditions:*** Block for review only if the Risk_scale is **0.9 or higher**, except in the case of the **LIFE_THREATENING_EMERGENCY** category.\n      \"\"\",\n      \"\"\"\n      ACTION NAME : Temporary Block with Review\n      - Temporary block with review team involvement: Block the user temporarily while still involving the review team, temp block should not exceed 10 min. Set `is_blocked = true` and `is_blocked_for_review = false` with the duration and timestamp, and escalate to the review team for further review and actions.\n        - ***Conditions:*** Used when the Risk_scale = 0.8, and the situation requires additional review.\n      \"\"\",\n      \"\"\"\n      ACTION NAME : Temporary Block  without Review\n      - Temporary block without review: Block the user temporarily for a set duration without involving the review team , temp block should not exceed 10 min. Set `is_blocked = true` with the appropriate duration and timestamp.\n        - ***Conditions:*** If the Risk_scale = 0.7 and the post is not in the  **LIFE_THREATENING_EMERGENCY**, or **PROMOTIONAL_CONTENT** categories, apply this action. This is a lower-risk block, but may still need to be escalated later.\n      \"\"\",\n      \"\"\"\n      **ACTION NAME : Non-Blocking only Escalation\n      - Escalate without blocking: If blocking isnâ€™t necessary, escalate the issue directly to the review team, emergency team, or security team without applying any block. This action is for situations where blocking the user is not required, but the issue needs to be addressed by another team. ensuring that the appropriate team (review, emergency, or security) is selected based on the context.\n        - ***Conditions:*** Use this action when you observe just escalation is enough, ensuring you select the most suitable team for the situation.\n      \"\"\"\n  ]","metadata":{"cellView":"form","id":"pyzl7anjHNO8","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:49:29.680055Z","iopub.execute_input":"2025-04-13T15:49:29.680383Z","iopub.status.idle":"2025-04-13T15:49:29.708508Z","shell.execute_reply.started":"2025-04-13T15:49:29.680351Z","shell.execute_reply":"2025-04-13T15:49:29.707333Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# ğŸ”§ Gemini Embedding Function for Vectorization\n* This class defines a custom embedding function using Google Gemini's text-embedding-004 model, which transforms input text into high-dimensional vectors. These embeddings are essential for performing semantic search and similarity comparisons within our moderation system.\n* Supports both document and query embedding modes using retrieval_document and retrieval_query task types.\n* Integrates with ChromaDB for efficient storage and retrieval of vectorized content.\n* Includes retry logic to handle temporary API failures gracefully.\n* This component powers the text understanding layer of our system, enabling downstream modules like RAG and content classification to work with vectorized representations of social media posts.","metadata":{}},{"cell_type":"code","source":"#Gemini Embedding Function for Vectorization\nclass GeminiEmbeddingFunction(EmbeddingFunction):\n    document_mode = True\n    def __init__(self):\n      print(\"Initializing Gemini emdeding function\")\n\n    @retry.Retry(predicate=is_retriable)\n    def __call__(self, input: Documents) -> Embeddings:\n        if self.document_mode:\n            embedding_task = \"retrieval_document\"\n        else:\n            embedding_task = \"retrieval_query\"\n\n        response = client.models.embed_content(\n            model=\"models/text-embedding-004\",\n            contents=input,\n            config=types.EmbedContentConfig(\n                task_type=embedding_task,\n            ),\n        )\n        return [e.values for e in response.embeddings]","metadata":{"cellView":"form","id":"AB2QzBSWHTw8","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:49:29.709601Z","iopub.execute_input":"2025-04-13T15:49:29.709976Z","iopub.status.idle":"2025-04-13T15:49:29.734142Z","shell.execute_reply.started":"2025-04-13T15:49:29.709936Z","shell.execute_reply":"2025-04-13T15:49:29.733211Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# ğŸ—‚ï¸ Local Database Service for Post Management and Moderation Logs\nThis cell defines a SQLite-based storage layer that acts as the persistent memory of our AI moderation system. It plays a vital role in capturing and maintaining user post information, moderation decisions, and visibility status for further auditing and review.\n\nâœ… Key Highlights:\nDatabaseService class handles:\n\n* Post ingestion with metadata like category, sentiment, tone, risk score, etc.\n* Moderation actions: blocking users, escalating posts, assigning visibility.\n* Structured tables: user_posts, blocked_user, review_posts, post_visibility_status.\n* Hashtag standardization and sanitization tracking.\n* PrintDataFromDataBase class:\n* Provides a beautifully formatted feed view with emojis, labels, and escalation highlights.\n* Clearly shows blocked status, review priorities, and original/sanitized post details.\n* This module ensures the system retains a traceable history of every user interaction and moderation action, making it reliable for future analysis, transparency, and compliance.","metadata":{}},{"cell_type":"code","source":"#Local Database Service for Post Management and Moderation Logs\nclass DatabaseService:\n  def __init__(self):\n    print(\"Initializing the database service\")\n    self.db_file = DataBaseName.get_database_name()\n    self.create_all_required_tables()\n\n  def create_all_required_tables(self):\n    conn = sqlite3.connect(self.db_file)\n    cursor = conn.cursor()\n    cursor.execute(\"DROP TABLE IF EXISTS user_posts\")\n    cursor.execute(\"DROP TABLE IF EXISTS blocked_user\")\n    cursor.execute(\"DROP TABLE IF EXISTS review_posts\")\n    cursor.execute(\"DROP TABLE IF EXISTS blocked_post\")\n    cursor.execute(\"DROP TABLE IF EXISTS post_visibility_status\")\n    cursor.execute(\"\"\"\n    CREATE TABLE user_posts (\n        post_id INTEGER PRIMARY KEY AUTOINCREMENT,\n        user_name VARCHAR(100),\n        post_txt TEXT,\n        category VARCHAR(50),\n        risk_scale DECIMAL(3,2),\n        organization_standards_applied VARCHAR(255),\n        organization_standards_applied_desc TEXT,\n        post_tone VARCHAR(50),\n        post_sentiment VARCHAR(50),\n        auto_hash_tag VARCHAR(255),\n        sanitized_post TEXT,\n        normalized_post TEXT,\n        actions_taken TEXT,\n        final_response_on_post_submission TEXT,\n        any_errors_while_executing_actions BOOLEAN\n    );\n    \"\"\")\n    cursor.execute(\"\"\"\n    CREATE TABLE blocked_user (\n      user_name VARCHAR(100) PRIMARY KEY,\n      is_blocked_for_review BOOLEAN DEFAULT 0,\n      is_blocked BOOLEAN DEFAULT 0,\n      block_duration INTEGER,\n      block_timestamp DATETIME\n    ); \"\"\")\n    cursor.execute(\"\"\"\n    CREATE TABLE review_posts (\n      post_id INTEGER PRIMARY KEY,\n      escalate_to_security_team BOOLEAN,\n      escalate_to_emergency_services BOOLEAN,\n      escalate_to_review_team BOOLEAN,\n      priority VARCHAR(50)\n    );\"\"\")\n    cursor.execute(\"\"\"\n    CREATE TABLE post_visibility_status (\n      post_id INTEGER PRIMARY KEY,\n      is_post_blocked BOOLEAN,\n      is_post_waiting_for_clearance BOOLEAN,\n      is_post_visible BOOLEAN\n    );\"\"\")\n    print(\"created required tables - done\")\n    conn.commit()\n    conn.close()\n\n  def insert_user_post_and_get_post_id(self,user_post_data)->int:\n    conn = sqlite3.connect(self.db_file)\n    cursor = conn.cursor()\n    post_data=json.loads(user_post_data)\n    insert_query = \"\"\"\n    INSERT INTO user_posts (\n           user_name, post_txt, category, risk_scale,\n          organization_standards_applied, organization_standards_applied_desc,\n          post_tone, post_sentiment, auto_hash_tag,\n          sanitized_post, normalized_post\n      ) VALUES ( ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n    \"\"\"\n\n    cursor.execute(insert_query, (\n            post_data[\"user_name\"],post_data[\"post_txt\"],\n            post_data[\"category\"],post_data[\"risk_scale\"],post_data[\"organization_standards_applied\"],\n            post_data[\"organization_standards_applied_desc\"],post_data[\"post_tone\"],post_data[\"post_sentiment\"],\n            self.auto_hashtag(post_data[\"auto_hash_tag\"]),post_data[\"sanitized_post\"],post_data[\"normalized_post\"]\n        ))\n\n    post_id = cursor.lastrowid\n    conn.commit()\n    conn.close()\n    return post_id\n\n  def block_user(\n      self,\n      user_name: str,\n      block_duration: int,\n      is_blocked_for_review:bool):\n    \"\"\"\n    Used to to block the user.\n    Args:\n        user_name:string -> user name of the user\n        block_duration:integer -> Number of minutes to block the user\n        is_blocked_for_review:boolean -> True if escalated to review team, this will make the post blocked untill review team review it.\n\n    Returns:\n        None\n    \"\"\"\n\n    conn = sqlite3.connect(self.db_file)\n    cursor = conn.cursor()\n\n    cursor.execute(\"\"\"\n    INSERT INTO blocked_user (user_name, is_blocked_for_review, is_blocked, block_duration, block_timestamp)\n    VALUES (?, ?, ?, ?, ?)\n    ON CONFLICT(user_name) DO UPDATE SET\n        is_blocked_for_review = excluded.is_blocked_for_review,\n        is_blocked = excluded.is_blocked,\n        block_duration = excluded.block_duration,\n        block_timestamp = excluded.block_timestamp\n    \"\"\", (\n        user_name,\n        is_blocked_for_review,\n        True,\n        block_duration,\n        datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    ))\n    conn.commit()\n    conn.close()\n\n  def add_review_entry(\n      self,\n      post_id:int,\n      escalate_to_security_team:bool,\n      escalate_to_emergency_services:bool,\n      escalate_to_review_team:bool,\n      priority:str):\n    \"\"\"\n    Used to escalate a post for individual team.\n    Args:\n        post_id:integer -> post id of the user post\n        escalate_to_security_team:boolean -> True if escalate to security team for app access issues and other security concerns in application, else False\n        escalate_to_emergency_services:boolean -> True if escalate to emergency services for quick help on emergency situvations else False\n        escalate_to_review_team:boolean -> True if escalate to review team for human review to user post and other concerns from user regarding application else False\n        priority:string -> priority of the the review any one in [LOW,MEDIUM,HIGH]\n\n    Returns:\n        None\n    \"\"\"\n    conn = sqlite3.connect(self.db_file)\n    cursor = conn.cursor()\n\n    cursor.execute(\"\"\"\n    INSERT INTO review_posts (\n        post_id,\n        escalate_to_security_team,\n        escalate_to_emergency_services,\n        escalate_to_review_team,\n        priority\n    ) VALUES (?, ?, ?, ?, ?)\n    \"\"\", (post_id, escalate_to_security_team, escalate_to_emergency_services, escalate_to_review_team,priority))\n\n    conn.commit()\n    conn.close()\n  import sqlite3\n\n  def insert_post_visibility_status(self,\n                                    post_id:int,\n                                    is_post_blocked:bool,\n                                    is_post_waiting_for_clearance:bool,\n                                    is_post_visible:bool):\n    \"\"\"\n    Used to determine post visibility status according to actions aplied.\n    Args:\n        post_id:integer -> post id of the user post\n        is_post_blocked:boolean -> True if the post has to be blocked else False\n        is_post_waiting_for_clearance:boolean -> True post is not blocked but waiting for clearance else False.\n        is_post_visible:boolean -> True if post is allowed to visible else False\n\n    Returns:\n        None\n    \"\"\"\n    conn = sqlite3.connect(self.db_file)\n    cursor = conn.cursor()\n\n    cursor.execute(\"\"\"\n            INSERT INTO post_visibility_status (\n                post_id,\n                is_post_blocked,\n                is_post_waiting_for_clearance,\n                is_post_visible\n            )\n            VALUES (?, ?, ?, ?)\n        \"\"\", (post_id, is_post_blocked, is_post_waiting_for_clearance, is_post_visible))\n\n    conn.commit()\n    conn.close()\n\n\n  def is_user_allowed_to_post(self,user_name):\n      conn = sqlite3.connect(self.db_file)\n      cursor = conn.cursor()\n\n      cursor.execute(\"\"\"\n          SELECT is_blocked_for_review, block_duration, block_timestamp\n          FROM blocked_user\n          WHERE user_name = ?\n      \"\"\", (user_name,))\n\n      row = cursor.fetchone()\n\n      if not row:\n          # No block entry exists, allow post\n          conn.close()\n          return True\n\n      is_blocked_for_review, block_duration, block_timestamp = row\n\n      if not is_blocked_for_review:\n          # Check if block duration has expired\n          if block_timestamp is None or block_duration is None:\n              conn.close()\n              return False  # Can't validate expiration\n          block_time = datetime.fromisoformat(block_timestamp)\n          now = datetime.now()\n          if (now - block_time).total_seconds() > block_duration * 60:\n              # Unblock the user by removing the record\n              cursor.execute(\"DELETE FROM blocked_user WHERE user_name = ?\", (user_name,))\n              conn.commit()\n              conn.close()\n              return True\n\n      conn.close()\n      return False\n  def update_actions_taken(self,post_id, actions):\n    conn = sqlite3.connect(self.db_file)\n    cursor = conn.cursor()\n    cursor.execute(\"\"\"\n            UPDATE user_posts\n            SET actions_taken = ?\n            WHERE post_id = ?\n        \"\"\", (actions, post_id))\n    conn.commit()\n    conn.close()\n\n\n  def update_final_response_to_user_on_post_submission(self,post_id, user_response, any_errors):\n    conn = sqlite3.connect(self.db_file)\n    cursor = conn.cursor()\n    cursor.execute(\"\"\"\n            UPDATE user_posts\n            SET final_response_on_post_submission = ?, any_errors_while_executing_actions=?\n            WHERE post_id = ?\n        \"\"\", (user_response,any_errors, post_id))\n    conn.commit()\n    conn.close()\n  def auto_hashtag(self,s):\n    if not s:\n        return s\n    return ','.join(\n        (word.strip() if word.strip().startswith('#') else f\"#{word.strip()}\")\n        .replace(' ', '_')\n        for word in s.split(',')\n    )\n\nclass PrintDataFromDataBase:\n  db_file = DataBaseName.get_database_name()\n  @staticmethod\n  def print_post_data():\n    conn = sqlite3.connect(PrintDataFromDataBase.db_file)\n    cursor = conn.cursor()\n    query = \"\"\"\n    with posts as (\n    SELECT\n        p.post_id as post_id,p.user_name as user_name,\n        ('\\U0001F4CC CATEGORY:' || p.category || ' & RISK:' || p.risk_scale) as designer_title,\n        ('#'|| p.post_id ||' \\U0001F60E' || '[' || user_name || ']') as designed_header,\n        (CASE\n          WHEN pv.is_post_waiting_for_clearance  = 1 THEN\n          '\\U0001F6AB Post has been waiting for review due to violation of organization policy: ' ||CHAR(10)||CHAR(9) ||\n           '\\U0001F4E3 ' || p.organization_standards_applied || ' - ' || p.organization_standards_applied_desc\n          ELSE COALESCE(NULLIF(sanitized_post, ''),NULLIF(normalized_post, ''),p.post_txt)\n        END) AS designed_post_txt,\n        (CASE \n        WHEN sanitized_post IS NOT NULL AND sanitized_post != '' THEN '\\U0001F6E1\\ufe0f Post has been sanitized'\n        WHEN normalized_post IS NOT NULL AND normalized_post != '' THEN '\\U0001F9EF Post has been normalized'\n        ELSE '\\U0001F4DC Original post (no sanitization or normalization)'\n    END) AS post_status,\n        ('\\u0023\\uFE0F\\u20E3' || p.auto_hash_tag) as designed_auto_hash_tag,\n        (CASE\n          WHEN category = 'LIFE_EMERGENCY' then\n             '\\u2757 Life Emergency content: Not sanitized to preserve original message for urgent review. Will be taken down if it violates any guidelines.'\n          ELSE ''\n        END) as non_filter_warnings,\n        p.category as category,p.risk_scale as risk_scale,\n        pv.is_post_blocked as is_post_blocked,pv.is_post_waiting_for_clearance as is_post_waiting_for_clearance,\n        pv.is_post_visible as is_post_visible\n    FROM\n        user_posts p\n    LEFT JOIN\n        post_visibility_status pv ON p.post_id = pv.post_id\n        where (pv.is_post_blocked is null or pv.is_post_blocked = 0)\n    ),\n    review_details as(\n      SELECT\n      post_id,\n      '\\U0001F6A8' || 'Post escalated to ' ||\n      TRIM(\n        CASE WHEN escalate_to_review_team = 1 THEN 'Review Team, ' ELSE '' END ||\n        CASE WHEN escalate_to_emergency_services = 1 THEN 'Emergency Services, ' ELSE '' END ||\n        CASE WHEN escalate_to_security_team = 1 THEN 'Security Team, ' ELSE '' END,\n        ', '\n      ) ||\n      ' with ' || priority || ' priority.' AS escalation_message\n    FROM review_posts\n    ),\n    post_display_data as (\n    select\n    posts.post_id,designer_title,designed_header,designed_post_txt,post_status,designed_auto_hash_tag,escalation_message,non_filter_warnings\n    from posts left join review_details on posts.post_id=review_details.post_id\n    )\n    select\n    (designed_header || CHAR(10) || designer_title || ' ' || post_status || CHAR(10) || CHAR(10) ||\n    '\\U0001F4A1 Thought Shared:' ||CHAR(10) ||\n    CHAR(9) || designed_post_txt || CHAR(10) ||CHAR(10) ||\n    designed_auto_hash_tag || CHAR(10) ||\n    (CASE\n      WHEN COALESCE(escalation_message, '') = '' THEN ''\n      ELSE escalation_message || CHAR(10)\n    END) ||\n    (CASE\n      WHEN COALESCE(non_filter_warnings, '') = '' THEN ''\n      ELSE non_filter_warnings || CHAR(10)\n    END) ||\n    '------------------------------------------------------------------------------------------------------------------------------------------------------') as `FEED POSTS`\n    from post_display_data\n    order by post_id desc\n    \"\"\"\n    cursor.execute(query)\n    rows = cursor.fetchall()\n    column_names = [description[0] for description in cursor.description]\n    print(tabulate(rows, headers=column_names))\n    conn.close()\n\n\n","metadata":{"cellView":"form","id":"sMvr89THHd2R","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:49:29.735383Z","iopub.execute_input":"2025-04-13T15:49:29.735666Z","iopub.status.idle":"2025-04-13T15:49:29.759631Z","shell.execute_reply.started":"2025-04-13T15:49:29.735646Z","shell.execute_reply":"2025-04-13T15:49:29.758745Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# ğŸš¦ MonitorService: Core Moderation Logic and Workflow Engine\nThe MonitorService class forms the heart of the moderation pipeline in our \"Social Media Posts Monitor and Controller\" system. It coordinates all core actions including:\n\n* ğŸ” Validation â€“ Checks for required fields and formats user posts.\n* ğŸ§  Category Classification â€“ Uses a Gemini-powered model to classify posts by category, tone, sentiment, and risk.\n* ğŸ§½ Sanitization and Normalization â€“ Depending on post category, applies content correction for safety and clarity.\n* ğŸ“Š RAG-based Risk & Action Logic â€“ Integrates organisational guidelines from ChromaDB for context-aware action decisions.\n* ğŸ›‘ Moderation Action Execution â€“ Calls predefined tools (via function calling) to take structured actions like blocking or escalating posts.\n* ğŸ“£ Final Review & Feedback â€“ Notifies users about moderation outcomes in a clear and structured response.\n\nThe system follows a modular flow driven by LangGraph agents and stateful processing. This class leverages ChromaDB, Geminiâ€™s function calling, and dynamic prompt templates to ensure high-quality, context-sensitive moderation actions.","metadata":{}},{"cell_type":"code","source":"#MonitorService: Core Moderation Logic and Workflow Engine\nclass MonitorService:\n\n  def __init__(self):\n    print(\"Initializing the application\")\n    self.chroma_client = chromadb.Client()\n    self.embed_fn = GeminiEmbeddingFunction()\n    self.databaseService = DatabaseService()\n    self.vector_db_guidelines_for_risk = self.chroma_client.get_or_create_collection(name=\"organisation_guidelines_for_risk\", embedding_function=self.embed_fn)\n    self.vector_db_guidelines_for_actions = self.chroma_client.get_or_create_collection(name=\"organisation_guidelines_for_actions\", embedding_function=self.embed_fn)\n    self.refresh_chroma_db_for_rag(OrganisationGuidelinesDocument.ORG_GUIDELINES_FOR_RISK,OrganisationGuidelinesDocument.ORG_GUIDELINES_FOR_ACTIONS)\n\n  def validate_user_post(self,state: ModerationState) -> ModerationState:\n    # print(\"validate_user_post\")\n    user_post = self.fix_unicode(state[\"user_post\"])\n    try:\n      user_post_dict = json.loads(user_post)\n      required_keys = [\"user_name\", \"post_txt\"]\n      for key in required_keys:\n        if key not in user_post_dict:\n          raise ValueError(f\"Missing key: {key}\")\n        if user_post_dict[key] is None or user_post_dict[key].strip() == \"\":\n          raise ValueError(f\"Value for {key} cannot be null or empty\")\n      user_post_dict[\"user_name\"] = self.fix_unicode(user_post_dict[\"user_name\"].upper().strip())\n      user_post_dict[\"post_txt\"] = self.fix_unicode(user_post_dict[\"post_txt\"].strip())\n      state[\"validated_post\"] =  json.dumps(user_post_dict, indent=4, ensure_ascii=False).strip()\n      return state\n    except json.JSONDecodeError:\n      raise ValueError(\"Invalid JSON format\")\n\n  def get_post_category(self,state: ModerationState) -> ModerationState:\n    # print(\"get_post_category\")\n    user_post = self.fix_unicode(state[\"validated_post\"])\n    user_post_dict = json.loads(user_post)\n    input = json.dumps({\"post_txt\": user_post_dict.get(\"post_txt\")}, indent=4, ensure_ascii=False)\n    response = client.models.generate_content(\n        model=Models.get_model(),\n        config=ModelConfig.get_category_model_config(),\n        contents=[Prompts.CATEGORY_ZERO_SHOT_PROMPT.format(self.retrive_guidelines_for_risk(user_post_dict.get(\"post_txt\")),input)]\n    )\n    response_txt = self.fix_unicode(response.text)\n    user_post_dict[\"category\"] = json.loads(response_txt)[\"category\"]\n    user_post_dict[\"risk_scale\"] = json.loads(response_txt)[\"risk_scale\"]\n    user_post_dict[\"organization_standards_applied\"] = json.loads(response_txt)[\"organization_standards_applied\"]\n    user_post_dict[\"organization_standards_applied_desc\"] = json.loads(response_txt)[\"organization_standards_applied_desc\"]\n    user_post_dict[\"post_tone\"] = json.loads(response_txt)[\"post_tone\"]\n    user_post_dict[\"post_sentiment\"] = json.loads(response_txt)[\"post_sentiment\"]\n    user_post_dict[\"auto_hash_tag\"] = json.loads(response_txt)[\"auto_hash_tag\"]\n    user_post_dict[\"normalized_post\"] = \"\"\n    user_post_dict[\"sanitized_post\"] =\"\"\n\n    result_json_str = json.dumps(user_post_dict, indent=4, ensure_ascii=False)\n    state[\"categorized_post\"] = result_json_str\n    return state\n\n  def normalize_user_post(self,state: ModerationState) -> ModerationState:\n    # print(\"normalize_user_post\")\n    user_post = self.fix_unicode(state[\"categorized_post\"])\n    user_post_dict = json.loads(user_post)\n    response = client.models.generate_content(\n        model=Models.get_model(),\n        config=ModelConfig.get_normalize_model_config(),\n        contents=[Prompts.NORMALIZE_FEW_SHOT_PROMOT.format(user_post_dict.get(\"post_txt\"))]\n    )\n    response_txt = self.fix_unicode(response.text)\n    user_post_dict[\"normalized_post\"] = json.loads(response_txt)[\"normalized_post\"]\n    result_json_str = json.dumps(user_post_dict, indent=4, ensure_ascii=False)\n    state[\"categorized_post\"]=result_json_str\n    return state\n\n  def sanitize_user_post(self,state: ModerationState) -> ModerationState:\n    # print(\"sanitize_user_post\")\n    user_post = self.fix_unicode(state[\"categorized_post\"])\n    user_post_dict = json.loads(user_post)\n    response = client.models.generate_content(\n        model=Models.get_model(),\n        config=ModelConfig.get_sanitize_model_config(),\n        contents=[Prompts.SANITIZE_FEW_SHOT_PROMPT.format(user_post_dict.get(\"post_txt\"))]\n    )\n    response_txt = self.fix_unicode(response.text)\n    user_post_dict[\"sanitized_post\"] = json.loads(response_txt)[\"sanitized_post\"]\n    result_json_str = json.dumps(user_post_dict, indent=4, ensure_ascii=False)\n    state[\"categorized_post\"] = result_json_str\n    return state\n\n  def take_actions_based_on_analysis(self,state: ModerationState)-> ModerationState:\n    # print(\"take_actions_based_on_analysis\")\n    user_post = self.fix_unicode(state[\"categorized_post\"])\n    user_post_dict = json.loads(user_post)\n    post_status=True\n\n    if (self.databaseService.is_user_allowed_to_post(user_post_dict.get(\"user_name\"))) or (user_post_dict.get(\"category\") in(PostAvilableCategory.LIFE_EMERGENCY.value,PostAvilableCategory.PRIORITY_SUPPORT.value)):\n      post_id = self.databaseService.insert_user_post_and_get_post_id(user_post)\n      user_post_dict[\"post_id\"] = post_id\n      if (user_post_dict.get(\"category\") != PostAvilableCategory.NORMAL.value) and (float(user_post_dict.get(\"risk_scale\"))>=0.5):\n        action_model_input = json.dumps(user_post_dict, indent=4, ensure_ascii=False)\n\n        #Non Rag based action model\n        response = client.models.generate_content(\n          model=Models.get_model(),\n          config=ModelConfig.get_action_model_config(self.databaseService.add_review_entry, self.databaseService.block_user, self.databaseService.insert_post_visibility_status),\n          contents=Prompts.APPLY_ACTIONS_NON_RAG_ZERO_SHOT_PROMPT.format(action_model_input,Prompts.ACTIONS_AVILABLE_ZERO_PROMPT)\n        )\n\n        #RAG based action Model\n        # response = client.models.generate_content(\n        #     model=Models.get_model(),\n        #     config=ModelConfig.get_rag_based_action_model_config(self.databaseService.add_review_entry,, self.databaseService.insert_post_visibility_status, self.databaseService.block_user,self.retrive_guidelines_for_actions),\n        #     contents = Prompts.APPLY_ACTIONS_RAG_ZERO_SHOT_PROMPT.format(action_model_input)\n        # )\n        response_txt = self.fix_unicode(response.text)\n        user_post_dict[\"actions_taken\"] = response_txt\n      else:\n        user_post_dict[\"actions_taken\"] = \"NO ACTIONS APPLIED, ITS POSTED\"\n      self.databaseService.update_actions_taken(user_post_dict[\"post_id\"],user_post_dict[\"actions_taken\"])\n    else:\n      user_post_dict[\"actions_taken\"] = \"User has been already blocked either for review or temp cool off block ! you are not allowed to post now ,sorry try again\"\n\n\n    result_json_str = json.dumps(user_post_dict, indent=4, ensure_ascii=False)\n    state[\"actioned_post\"] = result_json_str\n    return state\n\n  def review_action_applied_and_notify_user(self,state: ModerationState)->ModerationState:\n    user_post = self.fix_unicode(state[\"actioned_post\"])\n    user_post_dict = json.loads(user_post)\n    review_model_input = json.dumps(\n        {\n            \"user_name\": user_post_dict.get(\"user_name\"),\n            \"organization_standards_applied\": user_post_dict.get(\"organization_standards_applied\"),\n            \"organization_standards_applied_desc\": user_post_dict.get(\"organization_standards_applied_desc\"),\n            \"actions_taken\": user_post_dict.get(\"actions_taken\")\n         }, indent=4, ensure_ascii=False)\n    response = client.models.generate_content(\n        model=Models.get_model(),\n        config=ModelConfig.get_review_action_applied_and_notify_user_model_config(),\n        contents=[Prompts.REVIEW_ACTIONS_APPLIED_ZERO_SHOT_PROMPT.format(review_model_input)]\n    )\n    response_txt = self.fix_unicode(response.text)\n    user_post_dict[\"final_response_to_user_on_post_submission\"] = json.loads(response_txt)[\"final_response_to_user_on_post_submission\"]\n    user_post_dict[\"any_error_during_actions\"] = json.loads(response_txt)[\"any_error_during_actions\"]\n    if user_post_dict.get(\"post_id\"):\n      self.databaseService.update_final_response_to_user_on_post_submission(user_post_dict[\"post_id\"],user_post_dict[\"final_response_to_user_on_post_submission\"],user_post_dict[\"any_error_during_actions\"])\n    result_json_str = json.dumps(user_post_dict, indent=4, ensure_ascii=False)\n    state[\"final_output\"] = result_json_str\n    return state\n\n\n  def retrive_guidelines_for_risk(self,topic:str)->str:\n    query_text=\"\"\"Get me the rules related to text : {}\"\"\".format(topic)\n    result = self.vector_db_guidelines_for_risk.query(query_texts=query_text, n_results=3)\n    return \"\\n\".join(result[\"documents\"][0])\n\n  def retrive_guidelines_for_actions(self,topic:str)->str:\n    \"\"\"\n    Used to retrive document from the RAG.\n    Args:\n        topic:string -> search string from the vector db , always include catogory & risk_scale by default, others you can decide\n    Returns:\n        relavent guidlines as a string\n    \"\"\"\n    result = self.vector_db_guidelines_for_actions.query(query_texts=topic, n_results=2)\n    return \"\\n\".join(result[\"documents\"][0])\n\n  def refresh_chroma_db_for_rag(self,org_guidelines_for_risk,org_guidelines_for_actions):\n    self.embed_fn.document_mode = True\n    self.vector_db_guidelines_for_risk.add(documents=org_guidelines_for_risk, ids=[str(i) for i in range(len(org_guidelines_for_risk))])\n    self.vector_db_guidelines_for_actions.add(documents=org_guidelines_for_actions, ids=[str(i) for i in range(len(org_guidelines_for_actions))])\n    print(\"Updated chroma db with latest organisation policy\")\n    self.embed_fn.document_mode = False\n\n  def fix_unicode(self,data):\n    return data.encode('utf-8').decode('utf-8', errors='ignore')\n\n  def route_after_categorization(self,state: ModerationState) -> str:\n    post = json.loads(state[\"categorized_post\"])\n    if (post[\"category\"] in (\n        PostAvilableCategory.SENSITIVE.value,\n        PostAvilableCategory.OFFENSIVE_LANGUAGE.value,\n        PostAvilableCategory.PRIORITY_SUPPORT.value,\n        PostAvilableCategory.SPAM.value,\n        PostAvilableCategory.ADULT.value)) :\n        return \"Sanitize\"\n    elif post[\"category\"] in (\n        PostAvilableCategory.HARASSMENT.value,\n        PostAvilableCategory.THREAT.value\n        ) :\n        return \"Normalize\"\n    else:\n        return \"JusticeNode\"","metadata":{"id":"TkxuGD_iHlGy","cellView":"form","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:49:29.761702Z","iopub.execute_input":"2025-04-13T15:49:29.761989Z","iopub.status.idle":"2025-04-13T15:49:29.791262Z","shell.execute_reply.started":"2025-04-13T15:49:29.761960Z","shell.execute_reply":"2025-04-13T15:49:29.790420Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# ğŸ›¡ï¸ PostModerationEngine: Full Pipeline for Social Media Post Moderation\nThe PostModerationEngine class is the central engine that drives the entire moderation workflow in the Social Media Posts Monitor and Controller system. It ties together the different moderation services, orchestrating the entire process from post validation to user notification. Hereâ€™s a breakdown of its components and flow:\n\nService Initialization:\nThe MonitorService is instantiated to handle core moderation tasks like validating posts, categorizing them, and performing content normalization and sanitization.\n\nGraph-Oriented Workflow:\nThe moderation flow is structured as a State Graph, where each node represents a critical step in the moderation pipeline:\n\n* GateKeeper: Validates the user post, checking for necessary fields and formatting.\n* InsightEngine: Uses AI models to classify the post based on category, tone, sentiment, and risk.\n* Sanitize: Sanitizes the post for sensitive content when necessary.\n* Normalize: Normalizes content to ensure clarity and prevent misunderstanding.\n* JusticeNode: Determines whether any moderation actions are needed, like blocking or escalating the post.\n* VoiceBox: Sends feedback to the user, notifying them of moderation outcomes.\n\nConditional Routing:\nThe flow uses conditional edges to dynamically determine the next step based on the postâ€™s category (e.g., sensitive content may trigger sanitization).\n\nFinal Output:\nThe main method is the entry point to invoke the moderation flow. It takes a post_data string as input, runs it through the entire pipeline, and outputs the final feedback response for the user.\n\nGraph Visualization:\nThe get_graph() method generates a visual representation of the moderation workflow using Mermaid diagrams, providing an easy-to-understand map of the entire process.\n\nThis structured approach ensures a dynamic and flexible moderation system that can handle a wide range of social media posts, making it suitable for real-time content moderation at scale.","metadata":{}},{"cell_type":"code","source":"#PostModerationEngine: Full Pipeline for Social Media Post Moderation\nclass PostModerationEngine:\n    service = MonitorService()\n    graph = StateGraph(ModerationState)\n    \n    graph.add_node(\"GateKeeper\", service.validate_user_post)\n    graph.add_node(\"InsightEngine\", service.get_post_category)\n    graph.add_node(\"Sanitize\", service.sanitize_user_post)\n    graph.add_node(\"Normalize\", service.normalize_user_post)\n    graph.add_node(\"JusticeNode\", service.take_actions_based_on_analysis)\n    graph.add_node(\"VoiceBox\", service.review_action_applied_and_notify_user)\n    \n    graph.set_entry_point(\"GateKeeper\")\n    graph.add_edge(\"GateKeeper\", \"InsightEngine\")\n    graph.add_conditional_edges(\"InsightEngine\", service.route_after_categorization)\n    graph.add_edge(\"Sanitize\", \"JusticeNode\")\n    graph.add_edge(\"Normalize\", \"JusticeNode\")\n    graph.add_edge(\"JusticeNode\", \"VoiceBox\")\n    graph.add_edge(\"VoiceBox\", END)\n    graph\n    social_media_monitor_app = graph.compile()\n\n    @staticmethod\n    def main(post_data:str):\n        try:\n          result = PostModerationEngine.social_media_monitor_app.invoke({\"user_post\": user_post})\n          response = json.loads(result[\"final_output\"])\n          print(\"Post Response :\\n\"+response.get(\"final_response_to_user_on_post_submission\")+\"\\n\\n\")\n          PrintDataFromDataBase.print_post_data()\n        except ValueError as e:\n          print(e)\n    @staticmethod\n    def get_graph():\n        from IPython.display import Image, display\n        graph = PostModerationEngine.social_media_monitor_app.get_graph()\n        display(Image(graph.draw_mermaid_png()))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"cellView":"form","id":"OMRIMpDDL6nR","outputId":"d39a70e2-6d73-4c8c-8354-669311f416bf","jupyter":{"source_hidden":true},"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:49:29.792156Z","iopub.execute_input":"2025-04-13T15:49:29.792445Z","iopub.status.idle":"2025-04-13T15:49:30.942205Z","shell.execute_reply.started":"2025-04-13T15:49:29.792424Z","shell.execute_reply":"2025-04-13T15:49:30.941298Z"}},"outputs":[{"name":"stdout","text":"Initializing the application\nInitializing Gemini emdeding function\nInitializing the database service\ncreated required tables - done\nUpdated chroma db with latest organisation policy\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"ğŸ“ Testing the PostModerationEngine with a Sample Post\nLet's run the PostModerationEngine on a real-world example to see how it handles moderation tasks such as post validation, categorization, and taking appropriate actions.\n\nsample input = \n\n> {\n> \"user_name\": \"King\",\n> \"post_txt\": \"Hello everyone, I am in india, what a hevenly land\"\n> }\n\nsample output =\n\n* post response :\n* All Avilable Feed Post:","metadata":{}},{"cell_type":"code","source":"#Testing the PostModerationEngine with a Sample Post\nuser_post = \"\"\"\n{\n  \"user_name\": \"King\",\n  \"post_txt\": \"Hello everyone, I am in india, what a hevenly land\"\n}\n\"\"\"\nPostModerationEngine.main(user_post)\n# PostModerationEngine.get_graph()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"HVkq6I73wWib","outputId":"5ba7fd94-563c-4b1b-a71a-7dcd894671ec","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:49:30.943116Z","iopub.execute_input":"2025-04-13T15:49:30.943436Z","iopub.status.idle":"2025-04-13T15:49:32.912738Z","shell.execute_reply.started":"2025-04-13T15:49:30.943397Z","shell.execute_reply":"2025-04-13T15:49:32.911676Z"}},"outputs":[{"name":"stdout","text":"Post Response :\nHey KING, your post is visible. Just a friendly reminder to keep content genuine and avoid anything that might trigger false flags. Let's keep the community vibes positive!\n\n\nFEED POSTS\n------------------------------------------------------------------------------------------------------------------------------------------------------\n#1 ğŸ˜[KING]\nğŸ“Œ CATEGORY:NORMAL & RISK:0.1 ğŸ“œ Original post (no sanitization or normalization)\n\nğŸ’¡ Thought Shared:\n\tHello everyone, I am in india, what a hevenly land\n\n#ï¸âƒ£#travel_india,#explore_india,#visit_india\n------------------------------------------------------------------------------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":12}]}